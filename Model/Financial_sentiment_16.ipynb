{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6zctOdmHKLX",
        "outputId": "dd229fd3-4f58-4897-d4c5-c9a3112590de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Skipping tensorflow as it is not installed.\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eu8TbZuLizCK",
        "outputId": "9af05b75-8a0d-4fcc-e190-2eefd9c2c68f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: tensorflow in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.16.1)\n",
            "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow) (2.16.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.10.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (69.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.7)\n",
            "Requirement already satisfied: optree in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jerry\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2H2Wfz55yTO",
        "outputId": "e1805f0e-14bf-4e66-c4f3-67d32e073b7e"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Sentiment analysis using CNN on financial news\"\"\"\n",
        "\n",
        "# Install required libraries\n",
        "# !pip install contractions pandas nltk bs4 scikit-learn setuptools tensorflow \n",
        "\n",
        "# Import necessary libraries\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import tqdm\n",
        "import unicodedata\n",
        "import contractions\n",
        "from nltk.tokenize import word_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Activation, Dropout\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RxGt-ZDK_SE7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8fJneIO6CD4",
        "outputId": "97d9c71b-0b1d-4441-a316-af4d69f9ee25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\venv\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\venv\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\venv\\lib\\site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\venv\\lib\\site-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\venv\\lib\\site-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og-hagmh6Aya",
        "outputId": "61474794-c394-4e46-d084-647b59e11761"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\jerry\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\jerry\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7Zt06qZt-PAY"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess data\n",
        "df = pd.read_csv('../data.csv', delimiter=',', encoding='latin-1', header=None, skiprows=1)\n",
        "df = df.rename(columns=lambda x: ['Sentiment', 'Sentence'][x])\n",
        "df = df[['Sentence', 'Sentiment']]\n",
        "df = df[df.Sentiment != \"neutral\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "00IypqXrqf1N",
        "outputId": "aa2b448f-df57-42e5-ab14-5c980d0063de"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>positive</td>\n",
              "      <td>The GeoSolutions technology will leverage Bene...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>negative</td>\n",
              "      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>positive</td>\n",
              "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>neutral</td>\n",
              "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>neutral</td>\n",
              "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sentence                                          Sentiment\n",
              "0  positive  The GeoSolutions technology will leverage Bene...\n",
              "1  negative  $ESI on lows, down $1.50 to $2.50 BK a real po...\n",
              "2  positive  For the last quarter of 2010 , Componenta 's n...\n",
              "3   neutral  According to the Finnish-Russian Chamber of Co...\n",
              "4   neutral  The Swedish buyout firm has sold its remaining..."
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7iswhnMA4HR",
        "outputId": "ffae244e-a0e5-47a3-a914-b301d9390cbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: newspaper3k in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (0.2.8)\n",
            "Collecting lxml_html_clean\n",
            "  Using cached lxml_html_clean-0.1.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from newspaper3k) (4.12.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from newspaper3k) (10.3.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from newspaper3k) (6.0.1)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from newspaper3k) (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from newspaper3k) (5.2.1)\n",
            "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from newspaper3k) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from newspaper3k) (2.31.0)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from newspaper3k) (6.0.11)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from newspaper3k) (5.1.2)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from newspaper3k) (2.9.0.post0)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
            "Requirement already satisfied: six in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Requirement already satisfied: sgmllib3k in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: click in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.66.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2024.2.2)\n",
            "Requirement already satisfied: requests-file>=1.4 in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (2.0.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.13.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\jerry\\downloads\\sentiment_analysis-main\\sentiment_analysis-main\\.venv\\lib\\site-packages (from click->nltk>=3.2.1->newspaper3k) (0.4.6)\n",
            "Using cached lxml_html_clean-0.1.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: lxml_html_clean\n",
            "Successfully installed lxml_html_clean-0.1.1\n",
            "A panel discussion themed on \"Digital Economy in Integration and Innovation\" is held during the Boao Forum for Asia (BFA) Annual Conference 2024 in Boao, South China's Hainan province, March 28, 2024. [Photo/Xinhua]\n",
            "\n",
            "BOAO, Hainan - While highlighting the remarkable achievements unlocked by the Belt and Road Initiative (BRI), participants at the Boao Forum for Asia (BFA) Annual Conference 2024 also called on participating countries to strengthen cooperation in technological innovation and the digital economy.\n",
            "\n",
            "Since its inception in 2013, the China-proposed BRI has reaped substantial benefits from deepening the \"hard connectivity\" of infrastructure and the \"soft connectivity\" of technology, to coordination of standards and rules in its participating countries, said attendees at a panel discussion on high-quality Belt and Road cooperation on Friday.\n",
            "\n",
            "The BRI has launched thousands of projects in around 150 countries and regions over the past years, providing financial and technological assistance as well as bringing people tangible benefits, said Zafar Uddin Mahmood, policy advisor to the secretary general of the BFA.\n",
            "\n",
            "For Pakistan, one of the biggest beneficiaries of the BRI, former prime minister of Pakistan Shahid Khaqan Abbasi said that the initiative and the China-Pakistan Economic Corridor have promoted mutually beneficial cooperation in energy, transportation and other fields in the two countries.\n",
            "\n",
            "The purpose of the initiative is to promote shared development and prosperity for participating countries, said Renat Bekturov, governor of Astana International Financial Centre, adding that China is now leading technological innovation in many sectors, and sharing these technologies will be win-win for all the BRI participating countries.\n",
            "\n",
            "Data showed that from 2013 to 2022, the cumulative value of trade between China and BRI partner countries reached $19.1 trillion, with an average annual growth rate of 6.4 percent. Cumulative two-way investment between China and partner countries reached $380 billion, including $240 billion from China.\n",
            "\n",
            "While praising the fruitful results of the initiative thus far, participants at the panel discussion also emphasized the role of the \"Digital Silk Road,\" the technology dimension of the initiative, in promoting economic transformation and industrial restructuring in the BRI partner countries.\n",
            "\n",
            "The \"Digital Silk Road\" project is important for the overall development of BRI participating countries, as well as for the initiative itself, said Suma Chakrabarit, former president of the European Bank for Reconstruction and Development, noting that the project should strengthen cooperation and sharing of digital technology, help local people access digital technology, and enhance their innovation capability.\n",
            "\n",
            "His opinions were echoed by Jin Liqun, president and chair of the Board of Directors of the Asian Infrastructure Investment Bank, saying that the construction of digital infrastructure is critical, and project sponsors must consider the digital factor in the future investment.\n",
            "\n",
            "There exists a digital gap as low-income countries are weak in digital infrastructure and efforts should be made to improve their digital interconnection capability, he added.\n",
            "\n",
            "The world is facing various challenges during the promotion of digital technology such as artificial intelligence considering its legal and ethical uncertainties, which need the common efforts of BRI partner countries to deal with, said Abbasi.\n",
            "\n",
            "The BFA Annual Conference 2024 drew some 2,000 participants from March 26 to 29 in Boao, a resort town in Hainan province, South China. Founded in 2001, the BFA is a non-governmental and non-profit international organization committed to promoting regional economic integration and bringing Asian countries closer to their development goals.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install newspaper3k lxml_html_clean\n",
        "import newspaper\n",
        "# Create a newspaper article object\n",
        "article = newspaper.Article('https://www.chinadaily.com.cn/a/202403/29/WS66068ae1a31082fc043bf7a9.html')\n",
        "\n",
        "# Download the article\n",
        "article.download()\n",
        "\n",
        "# Parse the article\n",
        "article.parse()\n",
        "\n",
        "# Get the article text\n",
        "text = article.text\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09cj0IIR-W3x",
        "outputId": "6f27e70e-71e7-4c56-fd32-4e5dac21d446"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5842/5842 [00:04<00:00, 1200.44it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import newspaper\n",
        "# Create a newspaper article object\n",
        "article = newspaper.Article('https://www.chinadaily.com.cn/a/202403/29/WS66068ae1a31082fc043bf7a9.html')\n",
        "\n",
        "# Download the article\n",
        "article.download()\n",
        "\n",
        "# Parse the article\n",
        "article.parse()\n",
        "\n",
        "# Get the article text\n",
        "text = article.text\n",
        "\n",
        "# Save the article text to a file\n",
        "with open('financial_news.csv', 'w') as f:\n",
        "  f.write(text)\n",
        "# Data cleaning\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    [s.extract() for s in soup(['iframe', 'script'])]\n",
        "    stripped_text = soup.get_text()\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "    return stripped_text\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "def stopwords_removal(words):\n",
        "    list_stopwords = nltk.corpus.stopwords.words('english')\n",
        "    return [word for word in words if word not in list_stopwords]\n",
        "\n",
        "def pre_process_corpus(docs):\n",
        "    norm_docs = []\n",
        "    for doc in tqdm.tqdm(docs):\n",
        "        # Case folding\n",
        "        doc = doc.lower()\n",
        "        # Remove special characters and whitespaces\n",
        "        doc = strip_html_tags(doc)\n",
        "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "        doc = remove_accented_chars(doc)\n",
        "        doc = contractions.fix(doc)\n",
        "        doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
        "        doc = re.sub(' +', ' ', doc)\n",
        "        doc = doc.strip()\n",
        "        # Tokenize\n",
        "        doc = word_tokenize(doc)\n",
        "        # Filtering\n",
        "        doc = stopwords_removal(doc)\n",
        "        norm_docs.append(doc)\n",
        "\n",
        "    norm_docs = [\" \".join(word) for word in norm_docs]\n",
        "    return norm_docs\n",
        "\n",
        "df.Sentence = pre_process_corpus(df.Sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VrMEssUm5WRn",
        "outputId": "2c15abeb-117c-4895-f0ae-18cb4acac51d"
      },
      "outputs": [
        {
          "ename": "InvalidIndexError",
          "evalue": "Reindexing only valid with uniquely valued Index objects",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m y_encoded_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m y_encoded_df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Concatenate df_balance and y_encoded_df\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m df_balance \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_balance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_encoded_df\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Drop the original 'Sentence' column\u001b[39;00m\n\u001b[0;32m     21\u001b[0m df_balance\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentence\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\jerry\\Downloads\\Sentiment_Analysis-main\\Sentiment_Analysis-main\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\jerry\\Downloads\\Sentiment_Analysis-main\\Sentiment_Analysis-main\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:680\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m         obj_labels \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39maxes[\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ax]\n\u001b[0;32m    679\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_labels\u001b[38;5;241m.\u001b[39mequals(obj_labels):\n\u001b[1;32m--> 680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m \u001b[43mobj_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[0;32m    684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m concatenate_managers(\n\u001b[0;32m    685\u001b[0m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_axes, concat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm_axis, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m    686\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\jerry\\Downloads\\Sentiment_Analysis-main\\Sentiment_Analysis-main\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3885\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   3882\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_method(method, limit, tolerance)\n\u001b[0;32m   3884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n\u001b[1;32m-> 3885\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requires_unique_msg)\n\u001b[0;32m   3887\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   3888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp)\n",
            "\u001b[1;31mInvalidIndexError\u001b[0m: Reindexing only valid with uniquely valued Index objects"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming df contains your dataset\n",
        "\n",
        "# One-hot encode the target variable\n",
        "encoder = OneHotEncoder()\n",
        "y_encoded = encoder.fit_transform(df_balance[['Sentence']])\n",
        "y_encoded_df = pd.DataFrame(y_encoded.toarray(), columns=encoder.get_feature_names_out(['Sentence']))\n",
        "\n",
        "# Prefix column names to avoid duplicates\n",
        "y_encoded_df.columns = [f'sentiment_{col}' for col in y_encoded_df.columns]\n",
        "\n",
        "# Concatenate df_balance and y_encoded_df\n",
        "df_balance = pd.concat([df_balance, y_encoded_df], axis=1)\n",
        "\n",
        "# Drop the original 'Sentence' column\n",
        "df_balance.drop('Sentence', axis=1, inplace=True)\n",
        "\n",
        "# Handle imbalanced data (oversampling)\n",
        "smote = SMOTE(random_state=42)\n",
        "X = df_balance.drop(y_encoded_df.columns, axis=1)\n",
        "y = df_balance[y_encoded_df.columns]\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Split data into train and test sets after resampling\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=42)\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(X_train)\n",
        "vocab = len(token.index_word) + 1\n",
        "\n",
        "X_train = token.texts_to_sequences(X_train)\n",
        "X_test = token.texts_to_sequences(X_test)\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 30\n",
        "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
        "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "num_classes = 2\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_test = le.transform(y_test)\n",
        "\n",
        "# Build CNN model\n",
        "vec_size = 300\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab, vec_size))\n",
        "model.add(Conv1D(64, 8, activation=\"relu\"))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(8, activation=\"relu\"))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(4, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=tf.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "epochs = 100\n",
        "batch_size = 4\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "mc = ModelCheckpoint('./best_model/sentiment_analysis.keras', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, shuffle=True, validation_split=0.1, epochs=epochs, verbose=1, callbacks=[es, mc])\n",
        "\n",
        "# Evaluate model\n",
        "train_acc = model.evaluate(X_train, y_train, verbose=1)\n",
        "test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
        "print('Train: %.2f%%, Test: %.2f%%' % (train_acc[1]*100, test_acc[1]*100))\n",
        "\n",
        "# Identify overfitting\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Confusion matrix\n",
        "def predictions(x):\n",
        "    prediction_probs = model.predict(x)\n",
        "    predictions = [1 if prob > 0.5 else 0 for prob in prediction_probs]\n",
        "    return predictions\n",
        "\n",
        "labels = ['positive', 'negative']\n",
        "print(\"CNN 1D Accuracy: %.2f%%\" % (accuracy_score(y_test, predictions(X_test))*100))\n",
        "print(\"CNN 1D Precision: %.2f%%\" % (precision_score(y_test, predictions(X_test), average=\"macro\")*100))\n",
        "print(\"CNN 1D Recall: %.2f%%\" % (recall_score(y_test, predictions(X_test), average=\"macro\")*100))\n",
        "print(\"CNN 1D f1_score: %.2f%%\" % (f1_score(y_test, predictions(X_test), average=\"macro\")*100))\n",
        "print('================================================\\n')\n",
        "print(classification_report(y_test, predictions(X_test)))\n",
        "pd.DataFrame(confusion_matrix(y_test, predictions(X_test)), index=labels, columns=labels)\n",
        "\n",
        "# ROC AUC\n",
        "def plot_roc_curve(y_test, y_pred):\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "    plt.plot(fpr, tpr)\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "\n",
        "plot_roc_curve(y_test, predictions(X_test))\n",
        "print(\"Model AUC score: %.2f%%\" % (roc_auc_score(y_test, predictions(X_test))*100))\n",
        "\n",
        "# Save the trained model\n",
        "model.save('sentiment_analysis_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gbdDsz-vyFx",
        "outputId": "28911dbc-9237-4b1b-eb5d-404d8a01046b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
            "Sample 1: Negative\n",
            "Sample 2: Positive\n",
            "Sample 3: Positive\n",
            "Sample 4: Positive\n",
            "Sample 5: Negative\n",
            "Sample 6: Positive\n",
            "Sample 7: Negative\n",
            "Sample 8: Positive\n",
            "Sample 9: Negative\n",
            "Sample 10: Positive\n",
            "Sample 11: Negative\n",
            "Sample 12: Positive\n",
            "Sample 13: Negative\n",
            "Sample 14: Positive\n",
            "Sample 15: Negative\n",
            "Sample 16: Positive\n",
            "Sample 17: Negative\n",
            "Sample 18: Positive\n",
            "Sample 19: Negative\n",
            "Sample 20: Positive\n",
            "Sample 21: Negative\n",
            "Sample 22: Positive\n",
            "Sample 23: Negative\n",
            "Sample 24: Positive\n",
            "Sample 25: Negative\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define sentiment thresholds (modify these values if needed)\n",
        "POSITIVE_THRESHOLD = 0.5\n",
        "NEGATIVE_THRESHOLD = 1 - POSITIVE_THRESHOLD  # Threshold for negative sentiment\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('sentiment_analysis_model.h5')\n",
        "\n",
        "# Define sample text\n",
        "sample_text = []\n",
        "with open('financial_news.csv', 'r') as file:\n",
        "    for line in file:\n",
        "        sample_text.append(line)\n",
        "\n",
        "# Create tokenizer and fit on sample text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sample_text)\n",
        "\n",
        "# Define maximum sequence length based on your model's configuration (replace with actual value)\n",
        "MAX_SEQUENCE_LENGTH = 100  # Replace with appropriate value\n",
        "\n",
        "# Preprocess sample text\n",
        "sample_sequences = tokenizer.texts_to_sequences(sample_text)\n",
        "sample_padded = pad_sequences(sample_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"post\")\n",
        "\n",
        "# Predict sentiment\n",
        "predictions = model.predict(sample_padded)\n",
        "\n",
        "# Print results\n",
        "for i, prediction in enumerate(predictions):\n",
        "    if prediction[0] > POSITIVE_THRESHOLD:\n",
        "        print(f\"Sample {i+1}: Positive\")\n",
        "    elif prediction[0] < NEGATIVE_THRESHOLD:\n",
        "        print(f\"Sample {i+1}: Negative\")\n",
        "    else:\n",
        "        print(f\"Sample {i+1}: Neutral\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
